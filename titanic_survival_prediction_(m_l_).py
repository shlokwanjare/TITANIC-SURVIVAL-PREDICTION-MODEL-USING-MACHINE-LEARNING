# -*- coding: utf-8 -*-
"""Titanic Survival Prediction (M.L.).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GaDVxyPu4YbcTNFlVc5yHJmdgUMGSEKO

IMPORTING THE DEPENDENCIES/LIBRARIES
"""

import numpy as np                                                              #to create numpy arrays
import pandas as pd                                                             #to create data frames. data frames are structered frames where data is loaded for efficient analysis
import matplotlib.pyplot as pit                                                 #data visualization library. useful to make plot and graphs
import seaborn as sns                                                           #data visualization library. useful to make plot and graphs
from sklearn.model_selection import train_test_split                            #a function that is used to split data into training data and testing data
                                                                                #important machine learning algo containing several preprocessing functions as well as various machine learning algorithms
from sklearn.linear_model import LogisticRegression                             #logistic regression model
from sklearn.metrics import accuracy_score                                      #used for evaluating the model, to find how many predictions n stuff

"""DATA COLLECTION AND DATA PROCESSING"""

titanic_data = pd.read_csv('/content/Titanic-Dataset.csv')                      #loading the data from csv file to pandas dataframe

titanic_data.head()                                                             #head() function displays first 5 rows of data set

titanic_data.shape                                                              # .shape tells the no. of rows and columns (r,c)

titanic_data.info()                                                             #gets some more information of the dataset

titanic_data.isnull().sum()                                                     #gives the number of missing values in each column

"""TO HANDLE THE MISSING VALUES"""

#in the cabin column majority of values are missing, so nothing can particularly be done with it. Cant find the mean of the column and replace it
#hence better if we remove (drop) the column
titanic_data = titanic_data.drop(columns='Cabin', axis=1)                       #when dropping a row, axis=0; when dropping a column, axis=1
                                                                                #creating a new dataframe with the same name but only removing the cabin column

#replacing all the missing values in the Age column with the mean Age of all the values
titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)            #na means not available. now filling na values with the mean values. if we dont mention inplace=True it wont be saved in th original data frame for further operation

#dealing the embarked column
#lookout for the values repeated most of the time and replace the missing values in the embarked column with it. since they are in the form of categories mean of these values cannot be found. char files !!
#the most repeated values means the MODE values
print(titanic_data['Embarked'].mode())

print(titanic_data['Embarked'].mode()[0])

titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True) #replaced the missing value with the mode value in "Embarked" column

titanic_data.isnull().sum()                                                     #rechecking if all values are well updated

"""DATA ANALYSIS (TRY TO PLOT DATA IN FORM OF GRAPHS AND STUFF)"""

# getting some statistical measures about the data which is to be done in almsot every data set
titanic_data.describe()                                                         #describe() function gives the stat measures like mean std deviation etc
                                                                                #only applicable for the numeric value data
                                                                                #this isnt actually muh useful here since its a categorical data

titanic_data['Survived'].value_counts()                                         #finding the number of survivors

"""VISUALIZING THE DATA"""

sns.set()                                                                       #give a kind off theme to plot

sns.countplot(x='Survived', data=titanic_data)                                  #making a graph for survived column

sns.countplot(x='Sex', data=titanic_data)                                       #making a graph for sex column

titanic_data['Sex'].value_counts()                                              #finding the number of male and female survivors

sns.countplot(x='Sex', hue='Survived', data=titanic_data)                       #gender based survival plot

sns.countplot(x='Pclass', data=titanic_data)                                    #count plot for Pclass

sns.countplot(x='Pclass', hue='Survived', data=titanic_data)                    #class based survival plot

"""ENCODING THE CATEGORICAL COLUMNS (CONVERSION TO NUMERICAL VALUES)"""

titanic_data['Sex'].value_counts()

titanic_data['Embarked'].value_counts()

titanic_data.replace({'Sex':{'male':0,'female':1}, 'Embarked':{'S':0,'C':1,'Q':2}}, inplace=True)

titanic_data.head()

"""SEPERATING THE FEATURES AND TARGET"""

#survived is the target column and the rest are features
X = titanic_data.drop(columns=['PassengerId','Name','Ticket','Survived'],axis=1)
Y = titanic_data['Survived']

print(X)

print(Y)

"""SPLITTING THE DATA INTO TRAINING DATA AND TESTING DATA"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=2)  #creating 4 arrays for seperating the training and testing data
                                                                                       #corresponding values to x_train are stored in y_train similar for x_test and y_test
                                                                                       #test_size=0.2 means 80% of the data is used for train purpose and 20% only for test purpose
                                                                                       #random_state is used to split data in a common way, for code reprducibility

#following points are available for xtrain and xtest
print(X.shape, X_train.shape, X_test.shape)

"""MODEL TRAINING

LOGISTIC REGRESSION generally usefull for binary classification problem
"""

model = LogisticRegression()

#training the logistic regression model for data training
model.fit(X_train,Y_train)

"""EVALUATING THE MODEL

ACCURACY SCORE
"""

X_train_pred = model.predict(X_train)                                           #accuracy on training data

print(X_train_pred)

#accuracy score is nothing but comparing how many correct predictions are made by a model

training_data_accuracy = accuracy_score(Y_train, X_train_pred)

print('Accuracy score of training data : ',training_data_accuracy)

X_test_pred = model.predict(X_test)                                             #accuracy on test data

print(X_test_pred)

test_data_accuracy = accuracy_score(Y_test, X_test_pred)
print('Accuracy score of test data : ',test_data_accuracy)